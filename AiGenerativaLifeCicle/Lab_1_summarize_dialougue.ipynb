{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Set up Kernel and Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade pip   or python.exe -m pip install --upgrade pip\n",
    "# pip install --disable-pip-version-check\n",
    "#     torch==1.13.1\n",
    "#     torchdata==0.5.1 --quiet   (ajuda no carregamento de dados e em alguns outros aspectos específicos para conjunto de dados)\n",
    "# pip install\n",
    "#     transformers==4.27.2 (é uma bilioteca da Hugging Face, uma empresa de código aberto que criou muitas ferramentas de código aberto para grandes modelos de linguagem)\n",
    "#     datasets==2.11.0 --quiet (biblioteca python da Hugging Face, que pode ser carregado em muitos conjuntos de dados públicos que as pessoas usam para treinar modelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Summarize Dialouge without Prompt Engineering\n",
    "\n",
    "Neste caso de uso, vamos gerar resumo de um diálogo com o Large Language Model (LLM) FLAN-TS pré-treinado da Hugging Face. A lista de modelos disponíveis no pacote de transformadores da Hugging Face pode ser encontrada aqui.\n",
    "\n",
    "Vamos carregar alguns diálogos simples do conjunto de dados DialogSum Hugging Face. Esse conjunto de dados contém mais de 10.000 diálogos com os respectivos resumos e tópicos rotulados manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "# dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a couple of dialouges with theis baseline summaries\n",
    "# Imprimir alguns diálogos com esses resumos de linha de base\n",
    "\n",
    "examples_indices = [40, 200]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(examples_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE: ')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the FLAN-T5 model, creating an instance of the AutoModelForSeq2SeqLM class with .from_pretrained() method\n",
    "#Carregue o modelo FLAN-T5, criando uma instância da classe AutoModelForSeq2SeqLM com o método .from_pretrained()\n",
    "\n",
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar a codificação e a decodificação, você precisa trabalhar com o texto em um formato tokenizado. A tokenização é o processo de dividir o texto em unidades menores que podem ser processadas pelos modelos LLM.\n",
    "\n",
    "\n",
    "Faça o download do tokenizador para o modelo FLAN-T5 usando o método AutoTokenizer.from_pretrained(). O parâmetro use_fast ativa o tokenizador rápido. Nesta etapa, não há necessidade de entrar em detalhes, mas você pode encontrar os parâmetros do tokenizador na documentação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the tokenizer encoding and decoding a simple sentence:\n",
    "\n",
    "sentence =  \"What time is it, Tom?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors = 'pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"inputs_ids\"][0],\n",
    "        skip_special_tokens = True\n",
    ")\n",
    "\n",
    "print('ENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"inputs_is\"][0])\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora é hora de explorar a capacidade do LLM base de resumir um diálogo sem nenhuma engenharia de prompt. A engenharia de promp é o ato de um ser humano alterar o prompt (entrada) para melhorar a resposta de uma determinada tarefa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(examples_indices):\n",
    "    dialogue = datasets['test'][index]['dialogue']\n",
    "    summary = datasets['test'][index]['summary']\n",
    "\n",
    "    inputs = tokenizer(dialogue, return_tensors = 'pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"inputs_ids\"],\n",
    "            max_new_tokens = 50,\n",
    "        )[0],\n",
    "        skip_special_tokens = True\n",
    "    )\n",
    "\n",
    "print(dash_line)\n",
    "print('Example ', i + 1)\n",
    "print(dash_line)\n",
    "print('INPUT PROMPT:\\n{dialogue}')\n",
    "print(dash_line)\n",
    "print('BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n`{output}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode ver que as suposições do modelo fazem algum sentido, mas ele não parece ter certeza da tarefa que deve realizar. Parece que ele apenas inventa a próxima frase do diálogo. A engenharia imediata pode ajudar aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 - Summarize Dialouge with as Instruction Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 - Zero Shot Inference with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(examples_indices):\n",
    "    dialogue = datasets['test'][index]['dialogue']\n",
    "    summary = datasets['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the follwuing conversation.\n",
    "\n",
    "    {dialogue}\n",
    "\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    #Input constructed promp intead of dialogue\n",
    "    inputs = tokenizer(prompt, return_tensors = 'pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"inputs_ids\"],\n",
    "            max_new_tokens = 50,\n",
    "        )[0],\n",
    "        skip_special_tokens = True\n",
    "    )\n",
    "\n",
    "print(dash_line)\n",
    "print('Example ', i + 1)\n",
    "print(dash_line)\n",
    "print('INPUT PROMPT:\\n{dialogue}')\n",
    "print(dash_line)\n",
    "print('BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n`{output}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 - Zero Shot Inference with the Prompt Template from FLAN-T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, index in enumerate(examples_indices):\n",
    "    dialogue = datasets['test'][index]['dialogue']\n",
    "    summary = datasets['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Dialogue:\n",
    "\n",
    "    {dialogue}\n",
    "\n",
    "    What was going on?\n",
    "    \"\"\"\n",
    "\n",
    "    #Input constructed promp intead of dialogue\n",
    "    inputs = tokenizer(prompt, return_tensors = 'pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"inputs_ids\"],\n",
    "            max_new_tokens = 50,\n",
    "        )[0],\n",
    "        skip_special_tokens = True\n",
    "    )\n",
    "\n",
    "print(dash_line)\n",
    "print('Example ', i + 1)\n",
    "print(dash_line)\n",
    "print('INPUT PROMPT:\\n{dialogue}')\n",
    "print(dash_line)\n",
    "print('BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n`{output}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Summarize Dialouge with One Shot and Few Shot Inference\n",
    "    - 4.1 - One Shot Inference\n",
    "    - 4.2 Few Shot Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One shot e few shot inference é a prática de fornecer a um LLM um ou mais exemplos completos de pares de respostas a solicitações que correspondam à sua tarefa - antes da solicitação real que você deseja concluir. Isso é chamado de “in-context learning” e coloca seu modelo em um estado que compreende sua tarefa específica. Você pode ler mais sobre isso neste blog da HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_summarize):\n",
    "    prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 - Generative Configuration Parameters for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma função que receba uma lista de example_indices_full, gere um prompt com exemplos completos e, no final, anexe o prompt que você deseja que o modelo complete (example_index_to_summarize). Você usará o mesmo modelo de prompt do FLAN-T da seção 3.2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
